{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1Tj2Z6MN_P9W"
   },
   "source": [
    "# 6.4610 Research Project\n",
    "\n",
    "## Overview\n",
    "In this file, we implement a transformer model trained on OpenWebText"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MHmeYniHCpHz"
   },
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "spHhrEHz_P9X"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from typing import Tuple, Union, Optional, List, Dict\n",
    "import math\n",
    "import numpy as np\n",
    "from dataclasses import dataclass\n",
    "from transformers import AutoTokenizer\n",
    "import datasets\n",
    "from tqdm import tqdm\n",
    "import os\n",
    "import json\n",
    "from itertools import islice"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jDMbe1yh9SP4"
   },
   "source": [
    "## Preliminaries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 220
    },
    "executionInfo": {
     "elapsed": 23,
     "status": "error",
     "timestamp": 1763275034978,
     "user": {
      "displayName": "Yun Cheng Lin",
      "userId": "01553987611997725390"
     },
     "user_tz": 300
    },
    "id": "msTYZcSc-eoj",
    "outputId": "7f53bec7-35f8-4514-9865-454b5a45beef"
   },
   "outputs": [],
   "source": [
    "TRAIN_DATASETE_SIZE = 200000\n",
    "TEST_DATASET_SIZE = 10000\n",
    "TAYLOR_APPROXIMATION = 5\n",
    "PH_ALPHA = 0.5\n",
    "PH_SEMANTIC_HEADS = 3\n",
    "\n",
    "@dataclass\n",
    "class TransformerConfig:\n",
    "    \"\"\"Configuration class for transformer model\"\"\"\n",
    "    vocab_size: int = 50257\n",
    "    hidden_size: int = 768\n",
    "    num_attention_heads: int = 12\n",
    "    num_hidden_layers: int = 12\n",
    "    intermediate_size: int = 3072\n",
    "    max_position_embeddings: int = 512\n",
    "    use_causal_mask: bool = True\n",
    "    number_diffusion_kernels: int = 4\n",
    "\n",
    "@dataclass\n",
    "class TrainingConfig:\n",
    "    # Model hyperparameters\n",
    "    vocab_size: int = 50257\n",
    "    hidden_size: int = 768\n",
    "    num_attention_heads: int = 12\n",
    "    num_hidden_layers: int = 12\n",
    "    intermediate_size: int = 3072\n",
    "    max_position_embeddings: int = 512\n",
    "    use_causal_mask: bool = True\n",
    "\n",
    "    # Training hyperparameters\n",
    "    batch_size: int = 4\n",
    "    learning_rate: float = 5e-4\n",
    "    weight_decay: float = 0.01\n",
    "    num_epochs: int = 3\n",
    "    steps_per_epoch: int = 200000\n",
    "    warmup_steps: int = 1000\n",
    "    max_grad_norm: float = 1.0\n",
    "    save_steps: int = 10000\n",
    "    eval_steps: int = 5000\n",
    "    train_dataset_size: int = 200000\n",
    "    test_dataset_size: int = 10000\n",
    "\n",
    "    # Paths\n",
    "    output_dir: str = \"/transformer\"\n",
    "    log_dir: str = \"/logs_transformer\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "gGEqdw4n80aD"
   },
   "outputs": [],
   "source": [
    "class FeedForward(nn.Module):\n",
    "    def __init__(self, hidden_size: int, intermediate_size: int):\n",
    "        \"\"\"\n",
    "        Position-wise feed-forward network\n",
    "\n",
    "        Args:\n",
    "            hidden_size: Model dimension\n",
    "            intermediate_size: Hidden dimension of FFN\n",
    "            activation_fn: Activation function ('relu', 'gelu', etc.)\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "\n",
    "        self.activation = nn.GELU()\n",
    "        self.linear1 = nn.Linear(hidden_size, intermediate_size)\n",
    "        self.linear2 = nn.Linear(intermediate_size, hidden_size)\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        x = self.linear1(x)\n",
    "        x = self.activation(x)\n",
    "        x = self.linear2(x)\n",
    "        return x\n",
    "\n",
    "def count_parameters(model):\n",
    "    \"\"\"Count trainable parameters\"\"\"\n",
    "    return sum(p.numel() for p in model.parameters() if p.requires_grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "g8m_lg4P_P9Y"
   },
   "source": [
    "## Original Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "wv78Yip4_P9Z"
   },
   "outputs": [],
   "source": [
    "class RMSNorm(nn.Module):\n",
    "    def __init__(self, hidden_size: int, eps: float = 1e-6):\n",
    "        \"\"\"\n",
    "        RMS Normalization\n",
    "\n",
    "        Args:\n",
    "            hidden_size: The size of the hidden dimension\n",
    "            eps: Small constant for numerical stability\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.parameter = nn.Parameter(torch.ones(hidden_size))\n",
    "        self.eps = eps\n",
    "\n",
    "    def forward(self, hidden_states: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Apply RMS normalization\n",
    "\n",
    "        Args:\n",
    "            hidden_states: Input tensor of shape (batch_size, seq_len, hidden_size)\n",
    "\n",
    "        Returns:\n",
    "            Normalized tensor of shape (batch_size, seq_len, hidden_size)\n",
    "        \"\"\"\n",
    "        rms = torch.sqrt(torch.mean(torch.square(hidden_states), dim=-1, keepdim=True) + self.eps)\n",
    "        normalized = hidden_states / rms\n",
    "        return normalized * self.parameter\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "fjdU460W_P9b"
   },
   "outputs": [],
   "source": [
    "class AttentionHead(nn.Module):\n",
    "    def __init__(self, hidden_size: int, head_dim: int):\n",
    "        \"\"\"\n",
    "        Single attention head implementation\n",
    "\n",
    "        Args:\n",
    "            hidden_size: Input dimension\n",
    "            head_dim: Dimension of each attention head\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.head_dim = head_dim\n",
    "        self.WQ = nn.Linear(hidden_size, head_dim, bias=False)\n",
    "        self.WK = nn.Linear(hidden_size, head_dim, bias=False)\n",
    "        self.WV = nn.Linear(hidden_size, head_dim, bias=False)\n",
    "\n",
    "    def forward(self, x: torch.Tensor, attn_mask: Optional[torch.Tensor] = None) -> Tuple[torch.Tensor, torch.Tensor]:\n",
    "        \"\"\"\n",
    "        Forward pass for attention head\n",
    "\n",
    "        Args:\n",
    "            x: Input tensor (batch_size, seq_len, hidden_size)\n",
    "            attn_mask: Attention mask (batch_size, seq_len, seq_len) - 1 for attend, 0 for mask\n",
    "\n",
    "        Returns:\n",
    "            attention_output: (batch_size, seq_len, head_dim)\n",
    "            attention_weights: (batch_size, seq_len, seq_len)\n",
    "        \"\"\"\n",
    "        Q = self.WQ(x)\n",
    "        V = self.WV(x)\n",
    "        K = self.WK(x)\n",
    "        score = torch.matmul(Q, K.transpose(-2, -1)) / math.sqrt(self.head_dim)\n",
    "        if attn_mask is not None:\n",
    "          score = score.masked_fill(attn_mask == 0, -torch.inf)\n",
    "        attention_weights = torch.softmax(score, dim=-1)\n",
    "        attention_output = torch.matmul(attention_weights, V)\n",
    "        return attention_output, attention_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "atALg9xP_P9b"
   },
   "outputs": [],
   "source": [
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(self, hidden_size: int, num_heads: int):\n",
    "        \"\"\"\n",
    "        Multi-head attention implementation\n",
    "\n",
    "        Args:\n",
    "            hidden_size: Model dimension\n",
    "            num_heads: Number of attention heads\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        assert hidden_size % num_heads == 0, f\"The hidden size {hidden_size} is not divisible by the number of heads {num_heads}.\"\n",
    "        head_dim = hidden_size // num_heads\n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_heads = num_heads\n",
    "        self.head_dim = head_dim\n",
    "        self.heads = nn.ModuleList([AttentionHead(hidden_size, head_dim) for _ in range(num_heads)])\n",
    "        self.linear = nn.Linear(hidden_size, hidden_size, bias=False)\n",
    "\n",
    "    def forward(self, hidden_states: torch.Tensor, attention_mask: Optional[torch.Tensor] = None) -> Tuple[torch.Tensor, torch.Tensor]:\n",
    "        \"\"\"\n",
    "        Forward pass for multi-head attention\n",
    "\n",
    "        Args:\n",
    "            hidden_states: Input tensor (batch_size, seq_len, hidden_size)\n",
    "            attention_mask: Attention mask (batch_size, seq_len, seq_len)\n",
    "\n",
    "        Returns:\n",
    "            attention_output: (batch_size, seq_len, hidden_size)\n",
    "            attention_weights: (batch_size, num_heads, seq_len, seq_len)\n",
    "        \"\"\"\n",
    "        outputs = [each(hidden_states, attention_mask) for each in self.heads]\n",
    "        attention_outputs_tuple = tuple(each[0] for each in outputs)\n",
    "        attention_outputs = torch.stack(attention_outputs_tuple).transpose(0, 1).transpose(1, 2).flatten(2, 3)\n",
    "        attention_weights_tuple = tuple(each[1] for each in outputs)\n",
    "        attention_weights = torch.stack(attention_weights_tuple).transpose(0, 1)\n",
    "        attention_outputs = self.linear(attention_outputs)\n",
    "        return attention_outputs, attention_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "GGmyVRtR_P9c"
   },
   "outputs": [],
   "source": [
    "class TransformerBlock(nn.Module):\n",
    "    def __init__(self, hidden_size: int, num_heads: int, intermediate_size: int):\n",
    "        \"\"\"\n",
    "        Complete transformer block with attention and feed-forward\n",
    "\n",
    "        Args:\n",
    "            hidden_size: Model dimension\n",
    "            num_heads: Number of attention heads\n",
    "            intermediate_size: FFN hidden dimension\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.rms_att = RMSNorm(hidden_size=hidden_size)\n",
    "        self.rms_ffn = RMSNorm(hidden_size=hidden_size)\n",
    "        self.mha = MultiHeadAttention(hidden_size=hidden_size, num_heads=num_heads)\n",
    "        self.ffn = FeedForward(hidden_size=hidden_size, intermediate_size=intermediate_size)\n",
    "\n",
    "    def forward(self, hidden_states: torch.Tensor, attention_mask: Optional[torch.Tensor] = None) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Forward pass for transformer block\n",
    "\n",
    "        Args:\n",
    "            hidden_states: Input tensor (batch_size, seq_len, hidden_size)\n",
    "            attention_mask: Attention mask\n",
    "\n",
    "        Returns:\n",
    "            hidden_states: Output tensor (batch_size, seq_len, hidden_size)\n",
    "        \"\"\"\n",
    "        att_norm = self.rms_att(hidden_states)\n",
    "        self_att = self.mha(att_norm, attention_mask)[0]\n",
    "        res_conn_self_att = self_att + hidden_states\n",
    "        ffn_norm = self.rms_ffn(res_conn_self_att)\n",
    "        ffn_output = self.ffn(ffn_norm)\n",
    "        res_conn_ffn = res_conn_self_att + ffn_output\n",
    "        return res_conn_ffn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "iNKHVXuc_P9c"
   },
   "outputs": [],
   "source": [
    "def create_causal_mask(seq_len: int, device: torch.device) -> torch.Tensor:\n",
    "    \"\"\"Create a causal (lower triangular) attention mask\n",
    "\n",
    "    Args:\n",
    "        seq_len: Sequence length\n",
    "        device: Device to create the mask on\n",
    "\n",
    "    Returns:\n",
    "        Causal mask of shape (1, seq_len, seq_len)\n",
    "    \"\"\"\n",
    "    mask = torch.tril(torch.ones(seq_len, seq_len, device=device))\n",
    "    return mask.unsqueeze(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "H9xalskN_P9c"
   },
   "outputs": [],
   "source": [
    "class TransformerModel(nn.Module):\n",
    "    def __init__(self, config: TransformerConfig):\n",
    "        \"\"\"\n",
    "        Complete transformer model for causal language modeling\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.config = config\n",
    "        self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "        self.embeddings = nn.Embedding(num_embeddings=self.config.vocab_size, embedding_dim=self.config.hidden_size)\n",
    "        self.pos_embeddings = nn.Embedding(num_embeddings=self.config.max_position_embeddings, embedding_dim=self.config.hidden_size)\n",
    "        self.transformer = nn.ModuleList([TransformerBlock(hidden_size=self.config.hidden_size, num_heads=self.config.num_attention_heads, intermediate_size=self.config.intermediate_size)\n",
    "         for _ in range(self.config.num_hidden_layers)])\n",
    "        self.norm = RMSNorm(hidden_size=self.config.hidden_size)\n",
    "\n",
    "    def forward(self, input_ids: torch.Tensor, attention_mask: Optional[torch.Tensor] = None) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Forward pass for transformer model\n",
    "\n",
    "        Args:\n",
    "            input_ids: Token IDs (batch_size, seq_len)\n",
    "            attention_mask: Attention mask (batch_size, seq_len, seq_len)\n",
    "\n",
    "        Returns:\n",
    "            hidden_states: Final hidden states (batch_size, seq_len, hidden_size)\n",
    "        \"\"\"\n",
    "        positions = torch.arange(input_ids.shape[1], device=input_ids.device).unsqueeze(0).expand(input_ids.size(0), -1)\n",
    "        pos_embeds = self.pos_embeddings(positions)\n",
    "        token_embeddings = self.embeddings(input_ids) + pos_embeds\n",
    "        if attention_mask is None and self.config.use_causal_mask:\n",
    "          attention_mask = create_causal_mask(input_ids.shape[1], token_embeddings.device)\n",
    "        transf = token_embeddings\n",
    "        for layer in self.transformer:\n",
    "          transf = layer(transf, attention_mask=attention_mask)\n",
    "        output = self.norm(transf)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "wfYgs4iu_P9d"
   },
   "outputs": [],
   "source": [
    "class CausalLanguageModel(nn.Module):\n",
    "    def __init__(self, config: TransformerConfig):\n",
    "        \"\"\"Causal language model with transformer backbone\"\"\"\n",
    "        super().__init__()\n",
    "        self.config = config\n",
    "        self.transformer = TransformerModel(config)\n",
    "        self.lm_head = nn.Linear(config.hidden_size, config.vocab_size, bias=False)\n",
    "        self.criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "    def forward(self, input_ids: torch.Tensor, labels: Optional[torch.Tensor] = None) -> Union[torch.Tensor, Tuple[torch.Tensor, torch.Tensor]]:\n",
    "        \"\"\"\n",
    "        Forward pass for language model\n",
    "\n",
    "        Args:\n",
    "            input_ids: Token IDs (batch_size, seq_len)\n",
    "            labels: Target labels for loss computation (batch_size, seq_len)\n",
    "\n",
    "        Returns:\n",
    "            If labels provided: (loss, logits)\n",
    "            Else: logits only\n",
    "        \"\"\"\n",
    "        hidden_states = self.transformer(input_ids)\n",
    "        logits = self.lm_head(hidden_states)\n",
    "        if labels is not None:\n",
    "            logits_flat = logits[:, :-1, :].flatten(0, 1)\n",
    "            labels_flat = labels[:, 1:].flatten(0, 1)\n",
    "            return self.criterion(logits_flat, labels_flat), logits\n",
    "        return logits\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def generate(self, input_ids: torch.Tensor, max_new_tokens: int = 100, temperature: float = 1.0) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Generate text using the language model\n",
    "\n",
    "        Args:\n",
    "            input_ids: Starting token IDs (batch_size, seq_len)\n",
    "            max_new_tokens: Maximum number of tokens to generate\n",
    "            temperature: Sampling temperature\n",
    "\n",
    "        Returns:\n",
    "            Generated token IDs (batch_size, seq_len + max_new_tokens)\n",
    "        \"\"\"\n",
    "        for _ in range(max_new_tokens):\n",
    "            logits = self.forward(input_ids)[:, -1, :] / temperature\n",
    "            probs = nn.functional.softmax(logits, dim=-1)\n",
    "            next_token = torch.multinomial(probs, 1)\n",
    "            input_ids = torch.cat([input_ids, next_token], dim=1)\n",
    "        return input_ids\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kgksexR0xJA9"
   },
   "source": [
    "## Laplacian Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "X8H72L8sxPHV"
   },
   "outputs": [],
   "source": [
    "class RMSNorm(nn.Module):\n",
    "    def __init__(self, hidden_size: int, eps: float = 1e-6):\n",
    "        \"\"\"\n",
    "        RMS Normalization\n",
    "\n",
    "        Args:\n",
    "            hidden_size: The size of the hidden dimension\n",
    "            eps: Small constant for numerical stability\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.parameter = nn.Parameter(torch.ones(hidden_size))\n",
    "        self.eps = eps\n",
    "\n",
    "    def forward(self, hidden_states: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Apply RMS normalization\n",
    "\n",
    "        Args:\n",
    "            hidden_states: Input tensor of shape (batch_size, seq_len, hidden_size)\n",
    "\n",
    "        Returns:\n",
    "            Normalized tensor of shape (batch_size, seq_len, hidden_size)\n",
    "        \"\"\"\n",
    "        rms = torch.sqrt(torch.mean(torch.square(hidden_states), dim=-1, keepdim=True) + self.eps)\n",
    "        normalized = hidden_states / rms\n",
    "        return normalized * self.parameter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "29xUVYL9xXpR"
   },
   "outputs": [],
   "source": [
    "def matrix_exponential(laplacian):\n",
    "  \"\"\"\n",
    "  Approximation of the exponential of a matrix.\n",
    "\n",
    "  Args:\n",
    "    laplacian: Input tensor (batch_size, number_diffusion_kernels, seq_len, seq_len)\n",
    "  \"\"\"\n",
    "  batch_size, number_diffusion_kernels, seq_len, _ = laplacian.shape\n",
    "  device = laplacian.device\n",
    "\n",
    "  identity_matrix = torch.eye(seq_len, device=device)\n",
    "  laplacian_power = identity_matrix.unsqueeze(0).unsqueeze(0).repeat(batch_size, number_diffusion_kernels, 1, 1)\n",
    "\n",
    "  taylor_sum = torch.zeros_like(laplacian)\n",
    "\n",
    "  for l in range(TAYLOR_APPROXIMATION):\n",
    "      l_factorial_inv = 1.0 / math.factorial(l)\n",
    "      taylor_sum += laplacian_power * l_factorial_inv\n",
    "      laplacian_power = torch.matmul(laplacian_power, laplacian)\n",
    "\n",
    "  return taylor_sum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "cwbpP3Kvxacd"
   },
   "outputs": [],
   "source": [
    "class AttentionHeadL(nn.Module):\n",
    "    def __init__(self, hidden_size: int, head_dim: int, number_diffusion_kernels: int):\n",
    "        \"\"\"\n",
    "        Single attention head implementation\n",
    "\n",
    "        Args:\n",
    "            hidden_size: Input dimension\n",
    "            head_dim: Dimension of each attention head\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.head_dim = head_dim\n",
    "        self.number_diffusion_kernels = number_diffusion_kernels\n",
    "        self.WQ = nn.Linear(hidden_size, head_dim, bias=False)\n",
    "        self.WK = nn.Linear(hidden_size, head_dim, bias=False)\n",
    "        self.WV = nn.Linear(hidden_size, head_dim, bias=False)\n",
    "        self.WR = nn.Linear(hidden_size, head_dim, bias=False)\n",
    "\n",
    "        self.log_beta = nn.Parameter(torch.randn(number_diffusion_kernels))\n",
    "        self.weights = nn.Parameter(torch.rand(number_diffusion_kernels))\n",
    "        self.ratio_r = nn.Parameter(torch.randn(number_diffusion_kernels))\n",
    "\n",
    "    def forward(self, x: torch.Tensor, attn_mask: Optional[torch.Tensor] = None) -> Tuple[torch.Tensor, torch.Tensor]:\n",
    "        \"\"\"\n",
    "        Forward pass for attention head\n",
    "\n",
    "        Args:\n",
    "            x: Input tensor (batch_size, seq_len, hidden_size)\n",
    "            attn_mask: Attention mask (batch_size, seq_len, seq_len) - 1 for attend, 0 for mask\n",
    "\n",
    "        Returns:\n",
    "            attention_output: (batch_size, seq_len, head_dim)\n",
    "            attention_weights: (batch_size, seq_len, seq_len)\n",
    "        \"\"\"\n",
    "        seq_len = x.shape[1]\n",
    "        device = x.device\n",
    "\n",
    "        R = self.WR(x) # (batch_size, seq_len, head_dim)\n",
    "\n",
    "        dots = torch.matmul(R, R.transpose(-2, -1)) # (batch_size, seq_len, seq_len)\n",
    "        R_norms_sq = torch.sum(R * R, dim=-1, keepdim=True) # (batch_size, seq_len, 1)\n",
    "        squared_distance = R_norms_sq + R_norms_sq.transpose(-2, -1) - 2 * dots\n",
    "        distance = torch.sqrt(torch.relu(squared_distance)) # (batch_size, seq_len, seq_len)\n",
    "\n",
    "        self_loop_mask = (1.0 - torch.eye(seq_len, device=device)).bool()\n",
    "        identity_matrix = torch.eye(seq_len, device=device)\n",
    "\n",
    "        max_r = torch.amax(distance, (1, 2))\n",
    "        min_r = torch.amin(torch.where(self_loop_mask, distance, torch.inf), (1, 2))\n",
    "\n",
    "        ratio_r_expanded = self.ratio_r.view(1, self.number_diffusion_kernels)\n",
    "        restricted_ratio_r = torch.sigmoid(ratio_r_expanded)\n",
    "        range_r = (max_r - min_r).unsqueeze(-1)\n",
    "        min_r_expanded = min_r.unsqueeze(-1)\n",
    "        values_r = min_r_expanded + range_r * restricted_ratio_r\n",
    "\n",
    "        distance_expanded = distance.unsqueeze(1)\n",
    "        values_r_expanded = values_r.unsqueeze(-1).unsqueeze(-1)\n",
    "\n",
    "        mask = (distance_expanded < values_r_expanded) # (batch_size, number_diffusion_kernels, seq_len, seq_len)\n",
    "        adjacency = mask.float() * self_loop_mask.float()\n",
    "\n",
    "        degree_sums = torch.sum(adjacency, dim=-1).unsqueeze(-1)\n",
    "        laplacian = degree_sums * identity_matrix - adjacency\n",
    "\n",
    "        positive_beta = torch.nn.functional.softplus(self.log_beta)\n",
    "        beta_broadcastable = positive_beta.view(1, self.number_diffusion_kernels, 1, 1)\n",
    "        weighted_laplacian = -laplacian * beta_broadcastable\n",
    "        kernels = matrix_exponential(weighted_laplacian)\n",
    "\n",
    "        weights_broadcastable= self.weights.view(1, self.number_diffusion_kernels, 1, 1)\n",
    "        final_combined_kernel = torch.sum(kernels * weights_broadcastable, dim=1) # (batch_size, seq_len, seq_len)\n",
    "\n",
    "        Q = self.WQ(x)\n",
    "        V = self.WV(x)\n",
    "        K = self.WK(x)\n",
    "        score = torch.matmul(Q, K.transpose(-2, -1)) / math.sqrt(self.head_dim) + final_combined_kernel\n",
    "        if attn_mask is not None:\n",
    "            score = score.masked_fill(attn_mask == 0, -torch.inf)\n",
    "        attention_weights = torch.softmax(score, dim=-1)\n",
    "        attention_output = torch.matmul(attention_weights, V)\n",
    "\n",
    "        return attention_output, attention_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "qoBTrLyUxdD1"
   },
   "outputs": [],
   "source": [
    "class MultiHeadAttentionL(nn.Module):\n",
    "    def __init__(self, hidden_size: int, num_heads: int, number_diffusion_kernels: int):\n",
    "        \"\"\"\n",
    "        Multi-head attention implementation\n",
    "\n",
    "        Args:\n",
    "            hidden_size: Model dimension\n",
    "            num_heads: Number of attention heads\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        assert hidden_size % num_heads == 0, f\"The hidden size {hidden_size} is not divisible by the number of heads {num_heads}.\"\n",
    "        head_dim = hidden_size // num_heads\n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_heads = num_heads\n",
    "        self.head_dim = head_dim\n",
    "        self.number_diffusion_kernels = number_diffusion_kernels\n",
    "        self.heads = nn.ModuleList([AttentionHeadL(hidden_size, head_dim, number_diffusion_kernels) for _ in range(num_heads)])\n",
    "        self.linear = nn.Linear(hidden_size, hidden_size, bias=False)\n",
    "\n",
    "    def forward(self, hidden_states: torch.Tensor, attention_mask: Optional[torch.Tensor] = None) -> Tuple[torch.Tensor, torch.Tensor]:\n",
    "        \"\"\"\n",
    "        Forward pass for multi-head attention\n",
    "\n",
    "        Args:\n",
    "            hidden_states: Input tensor (batch_size, seq_len, hidden_size)\n",
    "            attention_mask: Attention mask (batch_size, seq_len, seq_len)\n",
    "\n",
    "        Returns:\n",
    "            attention_output: (batch_size, seq_len, hidden_size)\n",
    "            attention_weights: (batch_size, num_heads, seq_len, seq_len)\n",
    "        \"\"\"\n",
    "        outputs = [each(hidden_states, attention_mask) for each in self.heads]\n",
    "        attention_outputs_tuple = tuple(each[0] for each in outputs)\n",
    "        attention_outputs = torch.stack(attention_outputs_tuple).transpose(0, 1).transpose(1, 2).flatten(2, 3)\n",
    "        attention_weights_tuple = tuple(each[1] for each in outputs)\n",
    "        attention_weights = torch.stack(attention_weights_tuple).transpose(0, 1)\n",
    "        attention_outputs = self.linear(attention_outputs)\n",
    "        return attention_outputs, attention_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ixKRVcQ1xex3"
   },
   "outputs": [],
   "source": [
    "class TransformerBlockL(nn.Module):\n",
    "    def __init__(self, hidden_size: int, num_heads: int, intermediate_size: int, number_diffusion_kernels):\n",
    "        \"\"\"\n",
    "        Complete transformer block with attention and feed-forward\n",
    "\n",
    "        Args:\n",
    "            hidden_size: Model dimension\n",
    "            num_heads: Number of attention heads\n",
    "            intermediate_size: FFN hidden dimension\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.rms_att = RMSNorm(hidden_size=hidden_size)\n",
    "        self.rms_ffn = RMSNorm(hidden_size=hidden_size)\n",
    "        self.mha = MultiHeadAttentionL(hidden_size=hidden_size, num_heads=num_heads, number_diffusion_kernels=number_diffusion_kernels)\n",
    "        self.ffn = FeedForward(hidden_size=hidden_size, intermediate_size=intermediate_size)\n",
    "\n",
    "    def forward(self, hidden_states: torch.Tensor, attention_mask: Optional[torch.Tensor] = None) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Forward pass for transformer block\n",
    "\n",
    "        Args:\n",
    "            hidden_states: Input tensor (batch_size, seq_len, hidden_size)\n",
    "            attention_mask: Attention mask\n",
    "\n",
    "        Returns:\n",
    "            hidden_states: Output tensor (batch_size, seq_len, hidden_size)\n",
    "        \"\"\"\n",
    "        att_norm = self.rms_att(hidden_states)\n",
    "        self_att = self.mha(att_norm, attention_mask)[0]\n",
    "        res_conn_self_att = self_att + hidden_states\n",
    "        ffn_norm = self.rms_ffn(res_conn_self_att)\n",
    "        ffn_output = self.ffn(ffn_norm)\n",
    "        res_conn_ffn = res_conn_self_att + ffn_output\n",
    "        return res_conn_ffn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "UnWnjL_Uxt-p"
   },
   "outputs": [],
   "source": [
    "def create_causal_mask(seq_len: int, device: torch.device) -> torch.Tensor:\n",
    "    \"\"\"Create a causal (lower triangular) attention mask\n",
    "\n",
    "    Args:\n",
    "        seq_len: Sequence length\n",
    "        device: Device to create the mask on\n",
    "\n",
    "    Returns:\n",
    "        Causal mask of shape (1, seq_len, seq_len)\n",
    "    \"\"\"\n",
    "    mask = torch.tril(torch.ones(seq_len, seq_len, device=device))\n",
    "    return mask.unsqueeze(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "rpOingZ3xu8B"
   },
   "outputs": [],
   "source": [
    "class TransformerModelL(nn.Module):\n",
    "    def __init__(self, config: TransformerConfig):\n",
    "        \"\"\"\n",
    "        Complete transformer model for causal language modeling\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.config = config\n",
    "        self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "        self.embeddings = nn.Embedding(num_embeddings=self.config.vocab_size, embedding_dim=self.config.hidden_size)\n",
    "        self.pos_embeddings = nn.Embedding(num_embeddings=self.config.max_position_embeddings, embedding_dim=self.config.hidden_size)\n",
    "        self.transformer = nn.ModuleList([TransformerBlockL(hidden_size=self.config.hidden_size, num_heads=self.config.num_attention_heads,\n",
    "                                                           intermediate_size=self.config.intermediate_size, number_diffusion_kernels=self.config.number_diffusion_kernels)\n",
    "         for _ in range(self.config.num_hidden_layers)])\n",
    "        self.norm = RMSNorm(hidden_size=self.config.hidden_size)\n",
    "\n",
    "    def forward(self, input_ids: torch.Tensor, attention_mask: Optional[torch.Tensor] = None) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Forward pass for transformer model\n",
    "\n",
    "        Args:\n",
    "            input_ids: Token IDs (batch_size, seq_len)\n",
    "            attention_mask: Attention mask (batch_size, seq_len, seq_len)\n",
    "\n",
    "        Returns:\n",
    "            hidden_states: Final hidden states (batch_size, seq_len, hidden_size)\n",
    "        \"\"\"\n",
    "        positions = torch.arange(input_ids.shape[1], device=input_ids.device).unsqueeze(0).expand(input_ids.size(0), -1)\n",
    "        pos_embeds = self.pos_embeddings(positions)\n",
    "        token_embeddings = self.embeddings(input_ids) + pos_embeds\n",
    "        if attention_mask is None and self.config.use_causal_mask:\n",
    "          attention_mask = create_causal_mask(input_ids.shape[1], token_embeddings.device)\n",
    "        transf = token_embeddings\n",
    "        for layer in self.transformer:\n",
    "          transf = layer(transf, attention_mask=attention_mask)\n",
    "        output = self.norm(transf)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "9RIhVXFhxw-x"
   },
   "outputs": [],
   "source": [
    "class CausalLanguageModelL(nn.Module):\n",
    "    def __init__(self, config: TransformerConfig):\n",
    "        \"\"\"Causal language model with transformer backbone\"\"\"\n",
    "        super().__init__()\n",
    "        self.config = config\n",
    "        self.transformer = TransformerModelL(config)\n",
    "        self.lm_head = nn.Linear(config.hidden_size, config.vocab_size, bias=False)\n",
    "        self.criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "    def forward(self, input_ids: torch.Tensor, labels: Optional[torch.Tensor] = None) -> Union[torch.Tensor, Tuple[torch.Tensor, torch.Tensor]]:\n",
    "        \"\"\"\n",
    "        Forward pass for language model\n",
    "\n",
    "        Args:\n",
    "            input_ids: Token IDs (batch_size, seq_len)\n",
    "            labels: Target labels for loss computation (batch_size, seq_len)\n",
    "\n",
    "        Returns:\n",
    "            If labels provided: (loss, logits)\n",
    "            Else: logits only\n",
    "        \"\"\"\n",
    "        hidden_states = self.transformer(input_ids)\n",
    "        logits = self.lm_head(hidden_states)\n",
    "        if labels is not None:\n",
    "            logits_flat = logits[:, :-1, :].flatten(0, 1)\n",
    "            labels_flat = labels[:, 1:].flatten(0, 1)\n",
    "            return self.criterion(logits_flat, labels_flat), logits\n",
    "        return logits\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def generate(self, input_ids: torch.Tensor, max_new_tokens: int = 100, temperature: float = 1.0) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Generate text using the language model\n",
    "\n",
    "        Args:\n",
    "            input_ids: Starting token IDs (batch_size, seq_len)\n",
    "            max_new_tokens: Maximum number of tokens to generate\n",
    "            temperature: Sampling temperature\n",
    "\n",
    "        Returns:\n",
    "            Generated token IDs (batch_size, seq_len + max_new_tokens)\n",
    "        \"\"\"\n",
    "        for _ in range(max_new_tokens):\n",
    "            logits = self.forward(input_ids)[:, -1, :] / temperature\n",
    "            probs = nn.functional.softmax(logits, dim=-1)\n",
    "            next_token = torch.multinomial(probs, 1)\n",
    "            input_ids = torch.cat([input_ids, next_token], dim=1)\n",
    "        return input_ids\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "aX0AhvvnU-eY"
   },
   "source": [
    "##PH Model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "2kfs7Ob_XSsZ"
   },
   "outputs": [],
   "source": [
    "class PHDataset:\n",
    "    def __init__(self, dataset, tokenizer, max_length=512, max_samples=None):\n",
    "        self.dataset = dataset\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_length = max_length\n",
    "        self.max_samples = max_samples\n",
    "\n",
    "    def __iter__(self):\n",
    "        count = 0\n",
    "\n",
    "        for each in self.dataset:\n",
    "            text = each['text']\n",
    "\n",
    "            # Tokenize on the fly\n",
    "            encoded = self.tokenizer(\n",
    "                text,\n",
    "                truncation=True,\n",
    "                padding='max_length',\n",
    "                max_length=self.max_length,\n",
    "                return_tensors='pt'  # Return tensors for direct use\n",
    "            )\n",
    "\n",
    "            # Create labels (same as input_ids for causal language modeling)\n",
    "            labels = encoded['input_ids'].clone()\n",
    "\n",
    "            yield {\n",
    "                'input_ids': encoded['input_ids'].squeeze(0),  # Remove batch dimension\n",
    "                'labels': labels.squeeze(0)  # Remove batch dimension\n",
    "            }\n",
    "\n",
    "            count += 1\n",
    "            if self.max_samples is not None and count >= self.max_samples:\n",
    "                break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "OwhFoDnPXWso"
   },
   "outputs": [],
   "source": [
    "class PHRMSNorm(nn.Module):\n",
    "    def __init__(self, hidden_size: int, eps: float = 1e-6):\n",
    "        \"\"\"\n",
    "        RMS Normalization\n",
    "\n",
    "        Args:\n",
    "            hidden_size: The size of the hidden dimension\n",
    "            eps: Small constant for numerical stability\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.parameter = nn.Parameter(torch.ones(hidden_size))\n",
    "        self.eps = eps\n",
    "\n",
    "    def forward(self, hidden_states: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Apply RMS normalization\n",
    "\n",
    "        Args:\n",
    "            hidden_states: Input tensor of shape (batch_size, seq_len, hidden_size)\n",
    "\n",
    "        Returns:\n",
    "            Normalized tensor of shape (batch_size, seq_len, hidden_size)\n",
    "        \"\"\"\n",
    "        rms = torch.sqrt(torch.mean(torch.square(hidden_states), dim=-1, keepdim=True) + self.eps)\n",
    "        normalized = hidden_states / rms\n",
    "        return normalized * self.parameter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "_PDhfJwmXOCx"
   },
   "outputs": [],
   "source": [
    "class PHFeedForward(nn.Module):\n",
    "    def __init__(self, hidden_size: int, intermediate_size: int):\n",
    "        \"\"\"\n",
    "        Position-wise feed-forward network\n",
    "\n",
    "        Args:\n",
    "            hidden_size: Model dimension\n",
    "            intermediate_size: Hidden dimension of FFN\n",
    "            activation_fn: Activation function ('relu', 'gelu', etc.)\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "\n",
    "        self.activation = nn.GELU()\n",
    "        self.linear1 = nn.Linear(hidden_size, intermediate_size)\n",
    "        self.linear2 = nn.Linear(intermediate_size, hidden_size)\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        x = self.linear1(x)\n",
    "        x = self.activation(x)\n",
    "        x = self.linear2(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "0XeTiXH1W2n0"
   },
   "outputs": [],
   "source": [
    "class PHAttentionHead(nn.Module):\n",
    "    def __init__(self, hidden_size: int,\n",
    "                 head_dim: int,\n",
    "                 repn_dim: int = 64,  # Dimension of representation vectors\n",
    "                 k: int = 32,         # K-nearest neighbors (GLOBAL over batch)\n",
    "                 alpha: float = PH_ALPHA,  # Bias importance\n",
    "                 sim_mode: str = \"dot\",  # \"dot\" or \"l2\"\n",
    "                 semantic: bool = True    # False = normal head\n",
    "                 ):\n",
    "        super().__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.head_dim = head_dim\n",
    "        self.semantic = semantic\n",
    "\n",
    "        self.WQ = nn.Linear(hidden_size, head_dim, bias=False)\n",
    "        self.WK = nn.Linear(hidden_size, head_dim, bias=False)\n",
    "        self.WV = nn.Linear(hidden_size, head_dim, bias=False)\n",
    "\n",
    "        if semantic:\n",
    "            self.WR = nn.Linear(hidden_size, repn_dim, bias=False)\n",
    "            self.k = k\n",
    "            self.r = min(self.k, 32)  # small projection rank\n",
    "            self.U_raw = nn.Parameter(torch.randn(self.k, self.r) / math.sqrt(self.k))\n",
    "            self.alpha = alpha\n",
    "            self.sim_mode = sim_mode\n",
    "\n",
    "\n",
    "    def _mst_h0_lifetimes(self, dmats: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Batched Prim's algorithm to get H0 VR lifetimes (merge heights) from\n",
    "        distance matrices. Exact for H0. Vectorized over batch of neighborhoods.\n",
    "\n",
    "        dmats: (N, m, m) symmetric with 0 diag\n",
    "        returns: (N, m-1) lifetimes (MST edge weights)\n",
    "        \"\"\"\n",
    "        N, m, _ = dmats.shape\n",
    "        device = dmats.device\n",
    "\n",
    "        visited = torch.zeros(N, m, dtype=torch.bool, device=device)\n",
    "        visited[:, 0] = True\n",
    "\n",
    "        d_to_tree = dmats[:, 0, :].clone()  # (N, m)\n",
    "        d_to_tree[visited] = float('inf')\n",
    "\n",
    "        lifetimes = []\n",
    "        arangeN = torch.arange(N, device=device)\n",
    "\n",
    "        for _ in range(m - 1):\n",
    "            best_val, best_idx = d_to_tree.min(dim=1)   # (N,), (N,)\n",
    "            lifetimes.append(best_val)\n",
    "            visited[arangeN, best_idx] = True\n",
    "            new_row = dmats[arangeN, best_idx, :]       # (N, m)\n",
    "            d_to_tree = torch.minimum(d_to_tree, new_row)\n",
    "            d_to_tree[visited] = float('inf')\n",
    "\n",
    "        return torch.stack(lifetimes, dim=1)            # (N, m-1)\n",
    "\n",
    "    def forward(self, x: torch.Tensor, attn_mask: Optional[torch.Tensor] = None\n",
    "               ) -> Tuple[torch.Tensor, torch.Tensor]:\n",
    "        \"\"\"\n",
    "        x: (B, L, hidden_size)\n",
    "        attn_mask: (B, L, L)\n",
    "        \"\"\"\n",
    "        B, L, _ = x.shape\n",
    "\n",
    "        # standard attention\n",
    "        Q = self.WQ(x)  # (B, L, head_dim)\n",
    "        K = self.WK(x)  # (B, L, head_dim)\n",
    "        V = self.WV(x)  # (B, L, head_dim)\n",
    "        score = torch.matmul(Q, K.transpose(-2, -1)) / math.sqrt(self.head_dim)  # (B, L, L)\n",
    "\n",
    "        if self.semantic:\n",
    "            # PH features (H0 via MST) computed over the WHOLE BATCH\n",
    "            repn = self.WR(x)                                 # (B, L, repn_dim)\n",
    "            N = B * L\n",
    "            repn_flat = repn.reshape(N, -1).contiguous()      # (N, repn_dim)\n",
    "\n",
    "            with torch.no_grad():\n",
    "                # GLOBAL kNN across all tokens in the batch\n",
    "                D_all = torch.cdist(repn_flat, repn_flat, p=2)             # (N, N)\n",
    "                k_eff = min(self.k, max(1, N - 1))\n",
    "                knn_idx = D_all.topk(k_eff + 1, largest=False).indices     # (N, k+1), includes self\n",
    "\n",
    "                # neighborhoods and their pairwise distances\n",
    "                m = knn_idx.size(1)                                         # m = k_eff + 1\n",
    "                row = knn_idx.unsqueeze(-1).expand(-1, m, m)               # (N, m, m)\n",
    "                col = knn_idx.unsqueeze(-2).expand(-1, m, m)               # (N, m, m)\n",
    "                dmats = D_all[row, col].contiguous()\n",
    "\n",
    "            # H0 lifetimes via batched MST\n",
    "            lifetimes = self._mst_h0_lifetimes(dmats)  # (N, m-1) = (N, k_eff)\n",
    "\n",
    "            # pad to fixed length k and normalize\n",
    "            k_eff_actual = m - 1\n",
    "            if k_eff_actual < self.k:\n",
    "                pad = torch.zeros(N, self.k - k_eff_actual,\n",
    "                                   device=x.device, dtype=lifetimes.dtype)\n",
    "                lifetimes = torch.cat([lifetimes, pad], dim=1)  # (N, k)\n",
    "\n",
    "            Phi = torch.nn.functional.normalize(lifetimes, p=2, dim=-1).view(B, L, self.k)  # (B, L, k)\n",
    "\n",
    "            Phi_c = Phi - Phi.mean(dim=1, keepdim=True)                     # (B, L, k)\n",
    "            U = self.U_raw / (self.U_raw.norm(p='fro') + 1e-8)              # (k, r), scale-normalized\n",
    "            Z = torch.matmul(Phi_c, U)                                       # (B, L, r)\n",
    "            Z = torch.nn.functional.normalize(Z, p=2, dim=-1)               # (B, L, r)\n",
    "\n",
    "            # bias from Z\n",
    "            if self.sim_mode == \"dot\":\n",
    "                ph_bias = torch.bmm(Z, Z.transpose(1, 2))                   # (B, L, L)\n",
    "            else:  # \"l2\"\n",
    "                d = torch.cdist(Z, Z, p=2)\n",
    "                ph_bias = -(d ** 2)\n",
    "\n",
    "            score = score + self.alpha * ph_bias\n",
    "\n",
    "        if attn_mask is not None:\n",
    "            score = score.masked_fill(attn_mask == 0, float('-inf'))\n",
    "        attention_weights = torch.softmax(score, dim=-1)\n",
    "        attention_output = torch.matmul(attention_weights, V)\n",
    "        return attention_output, attention_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "furhOM4XW5-4"
   },
   "outputs": [],
   "source": [
    "class PHMultiHeadAttention(nn.Module):\n",
    "    def __init__(self, hidden_size: int, num_heads: int):\n",
    "        \"\"\"\n",
    "        Multi-head attention implementation\n",
    "\n",
    "        Args:\n",
    "            hidden_size: Model dimension\n",
    "            num_heads: Number of attention heads\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        assert hidden_size % num_heads == 0, f\"The hidden size {hidden_size} is not divisible by the number of heads {num_heads}.\"\n",
    "        head_dim = hidden_size // num_heads\n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_heads = num_heads\n",
    "        self.head_dim = head_dim\n",
    "        self.heads = nn.ModuleList([PHAttentionHead(hidden_size, head_dim, semantic=True) for __ in range(PH_SEMANTIC_HEADS)]+[PHAttentionHead(hidden_size, head_dim, semantic=False) for _ in range(num_heads-PH_SEMANTIC_HEADS)])\n",
    "        self.linear = nn.Linear(hidden_size, hidden_size, bias=False)\n",
    "\n",
    "    def forward(self, hidden_states: torch.Tensor, attention_mask: Optional[torch.Tensor] = None) -> Tuple[torch.Tensor, torch.Tensor]:\n",
    "        \"\"\"\n",
    "        Forward pass for multi-head attention\n",
    "\n",
    "        Args:\n",
    "            hidden_states: Input tensor (batch_size, seq_len, hidden_size)\n",
    "            attention_mask: Attention mask (batch_size, seq_len, seq_len)\n",
    "\n",
    "        Returns:\n",
    "            attention_output: (batch_size, seq_len, hidden_size)\n",
    "            attention_weights: (batch_size, num_heads, seq_len, seq_len)\n",
    "        \"\"\"\n",
    "        outputs = [each(hidden_states, attention_mask) for each in self.heads]\n",
    "        attention_outputs_tuple = tuple(each[0] for each in outputs)\n",
    "        attention_outputs = torch.stack(attention_outputs_tuple).transpose(0, 1).transpose(1, 2).flatten(2, 3)\n",
    "        attention_weights_tuple = tuple(each[1] for each in outputs)\n",
    "        attention_weights = torch.stack(attention_weights_tuple).transpose(0, 1)\n",
    "        attention_outputs = self.linear(attention_outputs)\n",
    "        return attention_outputs, attention_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "uD0ayqilWtpv"
   },
   "outputs": [],
   "source": [
    "class PHTransformerBlock(nn.Module):\n",
    "    def __init__(self, hidden_size: int, num_heads: int, intermediate_size: int):\n",
    "        \"\"\"\n",
    "        Complete transformer block with attention and feed-forward\n",
    "\n",
    "        Args:\n",
    "            hidden_size: Model dimension\n",
    "            num_heads: Number of attention heads\n",
    "            intermediate_size: FFN hidden dimension\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.rms_att = PHRMSNorm(hidden_size=hidden_size)\n",
    "        self.rms_ffn = PHRMSNorm(hidden_size=hidden_size)\n",
    "        self.mha = PHMultiHeadAttention(hidden_size=hidden_size, num_heads=num_heads)\n",
    "        self.ffn = PHFeedForward(hidden_size=hidden_size, intermediate_size=intermediate_size)\n",
    "\n",
    "    def forward(self, hidden_states: torch.Tensor, attention_mask: Optional[torch.Tensor] = None) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Forward pass for transformer block\n",
    "\n",
    "        Args:\n",
    "            hidden_states: Input tensor (batch_size, seq_len, hidden_size)\n",
    "            attention_mask: Attention mask\n",
    "\n",
    "        Returns:\n",
    "            hidden_states: Output tensor (batch_size, seq_len, hidden_size)\n",
    "        \"\"\"\n",
    "        att_norm = self.rms_att(hidden_states)\n",
    "        self_att = self.mha(att_norm, attention_mask)[0]\n",
    "        res_conn_self_att = self_att + hidden_states\n",
    "        ffn_norm = self.rms_ffn(res_conn_self_att)\n",
    "        ffn_output = self.ffn(ffn_norm)\n",
    "        res_conn_ffn = res_conn_self_att + ffn_output\n",
    "        return res_conn_ffn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "xoX0SQOpVEZ4"
   },
   "outputs": [],
   "source": [
    "class TransformerModelPH(nn.Module):\n",
    "    def __init__(self, config: TransformerConfig):\n",
    "        \"\"\"\n",
    "        Complete transformer model for causal language modeling\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.config = config\n",
    "        self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "        self.embeddings = nn.Embedding(num_embeddings=self.config.vocab_size, embedding_dim=self.config.hidden_size)\n",
    "        self.pos_embeddings = nn.Embedding(num_embeddings=self.config.max_position_embeddings, embedding_dim=self.config.hidden_size)\n",
    "        self.transformer = nn.ModuleList([PHTransformerBlock(hidden_size=self.config.hidden_size, num_heads=self.config.num_attention_heads, intermediate_size=self.config.intermediate_size)\n",
    "         for _ in range(self.config.num_hidden_layers)])\n",
    "        self.norm = PHRMSNorm(hidden_size=self.config.hidden_size)\n",
    "\n",
    "    def forward(self, input_ids: torch.Tensor, attention_mask: Optional[torch.Tensor] = None) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Forward pass for transformer model\n",
    "\n",
    "        Args:\n",
    "            input_ids: Token IDs (batch_size, seq_len)\n",
    "            attention_mask: Attention mask (batch_size, seq_len, seq_len)\n",
    "\n",
    "        Returns:\n",
    "            hidden_states: Final hidden states (batch_size, seq_len, hidden_size)\n",
    "        \"\"\"\n",
    "        positions = torch.arange(input_ids.shape[1], device=input_ids.device).unsqueeze(0).expand(input_ids.size(0), -1)\n",
    "        pos_embeds = self.pos_embeddings(positions)\n",
    "        token_embeddings = self.embeddings(input_ids) + pos_embeds\n",
    "        if attention_mask is None and self.config.use_causal_mask:\n",
    "          attention_mask = create_causal_mask(input_ids.shape[1], token_embeddings.device)\n",
    "        transf = token_embeddings\n",
    "        for layer in self.transformer:\n",
    "          transf = layer(transf, attention_mask=attention_mask)\n",
    "        output = self.norm(transf)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "wnMB4WM6VJd4"
   },
   "outputs": [],
   "source": [
    "class CausalLanguageModelPH(nn.Module):\n",
    "    def __init__(self, config: TransformerConfig):\n",
    "        \"\"\"Causal language model with transformer backbone\"\"\"\n",
    "        super().__init__()\n",
    "        self.config = config\n",
    "        self.transformer = TransformerModelPH(config)\n",
    "        self.lm_head = nn.Linear(config.hidden_size, config.vocab_size, bias=False)\n",
    "        self.criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "    def forward(self, input_ids: torch.Tensor, labels: Optional[torch.Tensor] = None) -> Union[torch.Tensor, Tuple[torch.Tensor, torch.Tensor]]:\n",
    "        \"\"\"\n",
    "        Forward pass for language model\n",
    "\n",
    "        Args:\n",
    "            input_ids: Token IDs (batch_size, seq_len)\n",
    "            labels: Target labels for loss computation (batch_size, seq_len)\n",
    "\n",
    "        Returns:\n",
    "            If labels provided: (loss, logits)\n",
    "            Else: logits only\n",
    "        \"\"\"\n",
    "        hidden_states = self.transformer(input_ids)\n",
    "        logits = self.lm_head(hidden_states)\n",
    "        if labels is not None:\n",
    "            logits_flat = logits[:, :-1, :].flatten(0, 1)\n",
    "            labels_flat = labels[:, 1:].flatten(0, 1)\n",
    "            return self.criterion(logits_flat, labels_flat), logits\n",
    "        return logits\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def generate(self, input_ids: torch.Tensor, max_new_tokens: int = 100, temperature: float = 1.0) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Generate text using the language model\n",
    "\n",
    "        Args:\n",
    "            input_ids: Starting token IDs (batch_size, seq_len)\n",
    "            max_new_tokens: Maximum number of tokens to generate\n",
    "            temperature: Sampling temperature\n",
    "\n",
    "        Returns:\n",
    "            Generated token IDs (batch_size, seq_len + max_new_tokens)\n",
    "        \"\"\"\n",
    "        for _ in range(max_new_tokens):\n",
    "            logits = self.forward(input_ids)[:, -1, :] / temperature\n",
    "            probs = nn.functional.softmax(logits, dim=-1)\n",
    "            next_token = torch.multinomial(probs, 1)\n",
    "            input_ids = torch.cat([input_ids, next_token], dim=1)\n",
    "        return input_ids\n"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": [
    {
     "file_id": "1fMwRFUiqLYBCOu0onyIxCNrmFvEotHIz",
     "timestamp": 1760365142371
    },
    {
     "file_id": "1scX-uB-gfC3WrbHpIjC53Bw3MnWbcXAh",
     "timestamp": 1758891234136
    },
    {
     "file_id": "1VTsBhtRyVzDxKzY7kkvO4WEn2WJWyhKS",
     "timestamp": 1758846551220
    },
    {
     "file_id": "17MRscB2ra1Dzg7kq5XEDTZy2MoOqQeQX",
     "timestamp": 1757966867897
    }
   ]
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
